import json
import os
from tqdm import tqdm
from utils import OpenAIModel
import argparse
import re
import sys
import concurrent.futures
import threading
import traceback
import tempfile
from filelock import FileLock

class GPT3_Reasoning_Graph_Baseline:
    def __init__(self, args):
        self.args = args
        self.data_path = args.data_path
        self.dataset_name = args.dataset_name
        self.split = args.split
        self.model_name = args.model_name
        self.save_path = args.save_path
        self.negation = args.negation
        self.mode = args.mode
        self.search_round = args.search_round
        self.file_lock = threading.Lock()
        self.batch_num = args.batch_num
        if args.base_url:
            self.openai_api = OpenAIModel(args.api_key, args.model_name, args.stop_words, args.max_new_tokens, args.reasoning_effort, base_url=args.base_url)
        else:
            self.openai_api = OpenAIModel(args.api_key, args.model_name, args.stop_words, args.max_new_tokens, args.reasoning_effort)
    
    def load_in_context_examples_complement(self):
        file_path = os.path.join('./prompts', self.dataset_name, 'complement_search.txt')
        with open(file_path) as f:
            in_context_examples = f.read()
        return in_context_examples
    
    def load_in_context_examples_logic_resolver(self):
        file_path = os.path.join('./prompts', self.dataset_name, 'logic_resolver.txt')
        with open(file_path) as f:
            in_context_examples = f.read()
        return in_context_examples
    
    
    def load_raw_dataset(self, split):
        if "llama" in self.model_name:
            model_name = 'llama'
        elif "Qwen3-14B" in self.model_name:
            model_name = 'qwen3-14b'
        elif "Qwen3-32B" in self.model_name:
            model_name = 'qwen3-32b'
        elif ":free" in self.model_name:
            model_name = self.model_name.replace(":free", "_free")
        elif "deepseek" in self.model_name:
            model_name = 'deepseek'
        else:
            model_name = self.model_name
        if self.negation == 'True':
            file_path = f"./results/{self.dataset_name}/{self.dataset_name}_{model_name}_trans_decompose_negated_data.json"
        else:
            file_path = f"./results/{self.dataset_name}/{self.dataset_name}_{model_name}_trans_decompose_no_negation.json"
        print(f"Loading raw dataset from {file_path}")
        
        with open(file_path, encoding='utf-8') as f:
            raw_dataset = json.load(f)
        return raw_dataset
        
    def remove_think_blocks(self, text):
        return re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()
    
    def index_context(self, context):
        sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?)\s', context)
        formatted_context = enumerate(sentences, start=1)
        indexed_sentences = '\n'.join([f"{index}: {sentence}" for index, sentence in formatted_context])
        return str(indexed_sentences)

    def construct_prompt_a(self, record, in_context_examples_trans):
        full_prompt = in_context_examples_trans
        context = record['context']
        question = record['question'].strip()
        full_prompt = full_prompt.replace('[[PREMISES]]', context)
        full_prompt = full_prompt.replace('[[CONJECTURE]]', question)
        return full_prompt

    def construct_prompt_b(self, responses_a, in_context_examples_decomposer):
        full_prompt = in_context_examples_decomposer
        full_prompt = full_prompt.replace('[[PREMISES]]', responses_a)
        return full_prompt

    def construct_prompt_c(self, responses_b, in_context_examples_search_init):
        full_prompt = in_context_examples_search_init
        full_prompt = full_prompt.replace('[[CONTEXT]]', responses_b)
        return full_prompt
    
    def construct_prompt_d(self, normalized_context, normalized_conjecture, negated_label, reasoning_step, sos_list, in_context_examples_search_router):
        full_prompt = in_context_examples_search_router
        full_prompt = full_prompt.replace('[[CONTEXT]]', normalized_context)
        full_prompt = full_prompt.replace('[[CONJECTURE]]', normalized_conjecture)
        full_prompt = full_prompt.replace('[[REASONING]]', reasoning_step)
        full_prompt = full_prompt.replace('[[SOS]]', sos_list)
        return full_prompt
    
    def construct_prompt_e(self, negated_label, conjecture, sos, selected_clause, in_context_examples_logic_resolver):
        full_prompt = in_context_examples_logic_resolver
        full_prompt = full_prompt.replace('[[SOS]]', sos)
        full_prompt = full_prompt.replace('[[SELECTED-CLAUSE]]', selected_clause)
        return full_prompt
    
    def construct_prompt_complement(self, sos, clause, in_context_examples_complement):
        full_prompt = in_context_examples_complement
        full_prompt = full_prompt.replace('[[SOS]]', sos)
        full_prompt = full_prompt.replace('[[CLAUSE]]', clause)
        return full_prompt
    
    def post_process_b(self, response_b):
        match = re.search(r"### Final Form:([\s\S]*)", response_b)
        if match:
            final_form_text = match.group(1)
            return final_form_text
        else:
            print("Text not found.")
            
    def extract_selected_clause(self, text):
        match = re.search(r'\[([^\[\]]+)\]', text)
        if match:
            selected_clause = match.group(1).strip()
            return selected_clause
        else:
            return "Selected clause not found."
        

    def post_process_c(self, response_c):
        sos_list = re.findall(r'\[(.*?)\]', response_c)
        negated_label = re.findall(r'\{(.*?)\}', response_c)
        str_sos_list = "".join(sos_list)
        str_negated_label = "".join(negated_label)
        return str_negated_label, str_sos_list
    
    def post_process_logic_solver(self, response_d):
        sufficiency_label_match = re.search(r'\[(.*?)\]', response_d)
        new_clause_match = re.search(r'\{(.*?)\}', response_d)
        
        new_clause = new_clause_match.group(1).strip() if new_clause_match else "New Clause not found."
        sufficiency_label = sufficiency_label_match.group(1).strip() if sufficiency_label_match else "Sufficiency Label not found."
        
        return {
            "new_clause": new_clause,
            "sufficiency_label": sufficiency_label,
        }
    
    def post_process_final_answer(self, response_c):
        pattern_bracket = r"Final answer: \{([A-E])\}"
        match = re.search(pattern_bracket, response_c)
        if match:
            answers =  match.group(1)
            return answers
        pattern_direct = r'\{(\w+)\}'
        match = re.search(pattern_direct, response_c, re.IGNORECASE)
        if match:
            return match.group(1).lower()
        return "No final answer found in the text."

    
    def final_process(self, final_answer):
        final_answer = final_answer.lower()
        if final_answer == "true":
            final_answer = 'A'
        elif final_answer == "false":
            final_answer = 'B'
        elif final_answer == "unknown":
            final_answer = 'C'
        else:
            final_answer = "No final answer found in the text."  
        return final_answer
    
    
    def clean_conjecture(self, conjecture):
        if isinstance(conjecture, dict):
            lines = []
            for key, value in conjecture.items():
                if not value:
                    continue

                if key == "Fact":
                    for line in value.splitlines():
                        if not line.strip():
                            continue
                        if ":::" in line:
                            left, right = line.split(":::", 1)
                            candidate = left if "(" in left else right
                            lines.append(candidate.strip())
                        elif "(" in line:
                            lines.append(line.strip())
                else:
                    lines.extend([l for l in value.splitlines() if l.strip()])

        else:
            lines = [l for l in conjecture.splitlines() if l.strip()]

        cleaned = []
        remove_list = ['Rules (others):', 'Rules (biconditional):', 'Rules (either_or):']
        for item in lines:
            if "(" in item and not any(rem in item for rem in remove_list):
                cleaned.append(item)

        return "\n".join(cleaned)
    
    def list_to_indexed_string(self, item_list):
        indexed_list = [f"{i + 1}. {item}" for i, item in enumerate(item_list)]
        return "\n".join(indexed_list)
    
    def extract_text_in_brackets(self, text):
        pattern = r'\[(.*?)\]'
        matches = re.findall(pattern, text)
        
        if matches:
            last_match = matches[-1]
            
            cleaned_match = last_match.lower().strip()
            
            if 'true' in cleaned_match:
                return 'true'
            elif 'false' in cleaned_match:
                return 'false'
        
        return None
    
    def is_complementary(self, term1, term2):
        pattern = r'(\w+)\((.*?)\)'
        
        match1 = re.search(pattern, term1)
        match2 = re.search(pattern, term2)
        
        if not match1 or not match2:
            return False
        
        predicate1, args1 = match1.group(1), match1.group(2).split(',')
        predicate2, args2 = match2.group(1), match2.group(2).split(',')
        
        if predicate1 != predicate2:
            return False
        
        if len(args1) != len(args2):
            return False
        
        for arg1, arg2 in zip(args1[:-1], args2[:-1]): 
            if arg1.strip() != arg2.strip() and arg1.strip() != 'x' and arg2.strip() != 'x':
                return False
        
        bool1 = args1[-1].strip()
        bool2 = args2[-1].strip()
        
        return (bool1 == 'True' and bool2 == 'False') or (bool1 == 'False' and bool2 == 'True')


    def filter_complementary_context(self, normalized_context, conjecture):

        def clean_and_extract_predicates(logical_statement):
            cleaned_statement = re.sub(r'[0-9.]', '', logical_statement)

            operator_pattern = r'(\\lor|\\land|\u2228|\u2227|\\wedge|\\vee)'
            terms = re.split(operator_pattern, cleaned_statement)
            return [term.strip() for term in terms]

        complementary_context_indices = []

        for idx, context in enumerate(normalized_context):
            terms = clean_and_extract_predicates(context)

            for term in terms:
                if self.is_complementary(term, conjecture):
                    complementary_context_indices.append(idx)
                    break 

        return complementary_context_indices

    def negate_boolean(self, expression):
        return re.sub(r"(False|True)", lambda x: "True" if x.group() == "False" else "False", expression, count=1)
    
    def ensure_bool_inside_parentheses(self, match):
        inner_content = match.group(1)
        terms = [term.strip() for term in inner_content.split(',')]
        if terms[-1] in ["True", "False"]:
            return f"({inner_content})"
        else:
            return f"({inner_content}, True)"
        
    def remove_negations(self, context):
        print('Negation Removal...', context)
        neg_list = ['\u00ac', '¬', '\\neg', '\\lnot', 'eg']
        
        context = re.sub(r"\(([^)]+)\)", self.ensure_bool_inside_parentheses, context)
        
        print('Negation Removal...', context)
        for neg in neg_list:
            while neg in context:
                neg_pos = context.find(neg)
                rest = context[neg_pos + len(neg):]
                bool_match = re.search(r"(false|true)", rest, flags=re.IGNORECASE)
                if bool_match:
                    bool_pos = bool_match.start()
                    bool_val = bool_match.group()
                    
                    new_bool = "True" if bool_val == "False" else "False"
                    
                    context = context[:neg_pos] + rest[:bool_pos] + new_bool + rest[bool_pos + len(bool_val):]
                else:
                    context = context[:neg_pos] + rest
        print('finish removal')
        return context
    
    def reasoning_graph_generation(self):
        raw_dataset = self.load_raw_dataset(self.split)
        print(f"Loaded {len(raw_dataset)} examples from {self.split} split.")
        
        in_context_examples_logic_resolver = self.load_in_context_examples_logic_resolver()
        
        counter = 0
    
        def process_example(example, counter):

            token_usage = {
                "prompt_tokens": 0,
                "completion_tokens": 0,
                "total_tokens": 0,
            }

            def add_usage(u):
                if not u:
                    return
                token_usage["prompt_tokens"] += u.get("prompt_tokens", 0)
                token_usage["completion_tokens"] += u.get("completion_tokens", 0)
                token_usage["total_tokens"] += u.get("total_tokens", 0)
                
            try:
                print(f"Running example: {example['id']}")

                flag = 'true'
                reasoning_step = []
                search_round = 0
                
                normalized_context = example['normalized_context']
                cleaned_normalized_context = self.clean_conjecture(normalized_context)
                normalized_context_list = cleaned_normalized_context.splitlines()
                normalized_context_list = [
                    '\n'.join(self.remove_negations(line) for line in item.replace('\\textnormal', '').replace('\\textrm', '').replace('\\\\text', '\\text')
                            .replace('\\text', '').replace('-', '') 
                            .replace('{', '').replace('}', '').replace('\\right', '')
                            .replace('\\left', '').replace('\\newline', '\n').replace('$', '').split('\n'))
                    for item in normalized_context_list if "(" in item and ")" in item
                ]

                
                normalized_conjecture = self.clean_conjecture(example['normalized_conjecture'])
                negated_label = example['negated_label']
                raw_sos = example.get('sos_list', "")
                sos_list = self.clean_conjecture(raw_sos)
                if not isinstance(sos_list, str):
                    sos_list = ""
                if str(example.get('negated_label', '')).lower() == 'true':
                    sos_list = normalized_conjecture
                    sos_list = self.remove_negations(sos_list)
                    if example['negated_label'] == 'True':
                        sos_list = self.negate_boolean(sos_list)
                        
                modified_context_list = []
                for item in normalized_context_list:
                    item = next((p.strip() for p in item.split(":::", 1) if "(" in p), "")
                    if '\\wedge' in item:
                        split_items = item.split('\\wedge')
                        modified_context_list.extend([sub_item.strip() for sub_item in split_items])
                    elif '\\land' in item:
                        split_items = item.split('\\land')
                        modified_context_list.extend([sub_item.strip() for sub_item in split_items])
                    elif '\u2227' in item:
                        split_items = item.split('\u2227')
                        modified_context_list.extend([sub_item.strip() for sub_item in split_items]) 
                    else:
                        modified_context_list.append(item)
                normalized_context_list = [item for item in modified_context_list if "(" in item and ")" in item]
                print("Normalized Context List: ", normalized_context_list)
                print("Normalized Conjecture: ", normalized_conjecture)
                print("sos_list: ", sos_list)
                
                normalized_context_list.append(sos_list)
                
                list_of_sos = []
                list_of_compelment = []
                
                selected_clause = None
                    
                while flag == 'true':
                    if search_round >= self.search_round:
                        final_answer = "No final answer found in the text."
                        break
                    
                    if selected_clause == None:
                        print("Search Router Operating...")
                        print("Running: ", example['id'])
                        print("Ground truth: ", example['ground_truth'])
                        
                        complement_indices = self.filter_complementary_context(normalized_context_list, sos_list)
                        
                        if complement_indices:
                            potential_clauses = [normalized_context_list[index] for index in complement_indices]
                            valid_clauses = sorted(
                                [clause for clause in potential_clauses if not any(step[0] == sos_list and step[1] == clause for step in reasoning_step)],
                                key=len
                            )
                            print("Potential Clauses: ", potential_clauses)
                            print("Valid Clauses: ", valid_clauses)
                            
                            if valid_clauses:
                                if len(valid_clauses) > 1:
                                    list_of_sos.append(sos_list)
                                    list_of_compelment.append(valid_clauses)
                                    print("Current SOS: ", sos_list, "Current Complement: ", valid_clauses)
                                    selected_clause = list_of_compelment[-1].pop(0)
                                else:
                                    selected_clause = valid_clauses[0]
                            else:
                                print("All potential clauses have been used before with this SOS list.")
                                print("Checking cached SOS and complement pairs.")
                                found_new_pair = False
                                if len(list_of_compelment) > 0:
                                    for i, complement_clauses in enumerate(list_of_compelment):
                                        if complement_clauses:
                                            sos_list = list_of_sos[i]
                                            selected_clause = complement_clauses.pop(0)
                                            found_new_pair = True
                                            break
                                        
                                    if not found_new_pair:
                                        print("No more sos and complement pairs found in cache.")
                                        final_answer = "cannot find sos with complement"
                                        break
                                else:
                                    print("No sos and complement pairs found in cache.")
                                    final_answer = "cannot find sos with complement"
                                    flag = 'false'
                                    break
                                    
                        if not complement_indices:
                            if len(list_of_compelment) > 0:
                                all_empty = True
                                original_sos = sos_list
                                original_selected_clause = selected_clause
                                for i, clauses in enumerate(list_of_compelment):
                                    if len(clauses) > 0:
                                        new_selected_clause = list_of_compelment[i][0]
                                        new_sos_list = list_of_sos[i]
                                        if new_sos_list != original_sos or new_selected_clause != original_selected_clause:
                                            selected_clause = list_of_compelment[i].pop(0)
                                            sos_list = new_sos_list
                                            all_empty = False 
                                            print("Check cache: ", sos_list, "Current Complement: ", selected_clause)
                                            break
                                if all_empty:
                                    final_answer = "No complement found in both context and cache."
                                    break
                            else: 
                                final_answer = "No complement found in the context."
                                break

                    if any(step[0].strip() == sos_list.strip() and step[1].strip() == selected_clause.strip() for step in reasoning_step):
                        print("Skipping this search round as it has appeared before.")
                        found_new_pair = False
                        for i, complement_clauses in enumerate(list_of_compelment):
                            if complement_clauses:
                                new_sos_list = list_of_sos[i]
                                new_selected_clause = complement_clauses[0]
                                if new_sos_list != sos_list or new_selected_clause != selected_clause:
                                    sos_list = new_sos_list
                                    selected_clause = complement_clauses.pop(0)
                                    found_new_pair = True
                                    break

                        if not found_new_pair:
                            print("No more sos and complement pairs found in cache.")
                            final_answer = "No sos and complement found"
                            break 
                    else:
                        print("SOS: ", sos_list)
                        print("Selected Clause: ", selected_clause)
                        print("Search round: ", search_round)
                        
                    prompts_e = self.construct_prompt_e(negated_label, normalized_conjecture, sos_list, selected_clause, in_context_examples_logic_resolver)
                    responses_e, _ , usage_e = self.openai_api.generate(prompts_e)
                    add_usage(usage_e)
                    # Get rid the text of any <think> ... </think> block from the raw model output
                    responses_e = self.remove_think_blocks(responses_e)
                    logic_solver_result = self.post_process_logic_solver(responses_e)
                    new_clause = logic_solver_result['new_clause']
                    sufficiency_label = logic_solver_result['sufficiency_label']
                    
                    solve_step = [sos_list, selected_clause, new_clause]
                    
                    reasoning_step.append(solve_step)
                    print('Reasoning Steps:')
                    for step in reasoning_step:
                        sos_list, selected_clause, new_clause = step
                        solving_step = f"SOS clause: {sos_list}. Selected Clause: {selected_clause}. New Clause: {new_clause}"
                        print(solving_step)
                    
                    if not new_clause.strip() or new_clause == "New Clause not found.":
                        print("No new clause found. Searhing from the cache.")
                        all_empty = "True"
                        for i, clause in enumerate(list_of_compelment):
                            if len(clause) > 0:
                                selected_clause = list_of_compelment[i].pop(0)
                                sos_list = list_of_sos[i]
                                all_empty = "False"
                                print("Searching from cache：Current SOS: ", sos_list, "Current Complement: ", selected_clause)
                                break
                                
                        if len(list_of_compelment) > 0 and all_empty == "True": 
                            final_answer = "Unknown"
                            flag = 'false'
                        elif len(list_of_compelment) == 0:
                            final_answer = "Unknown"
                            flag = 'false'
                    else:
                        sos_list = new_clause
                        normalized_context_list.append(new_clause)
                        selected_clause = None
                    
                    if sufficiency_label == "True":
                        if new_clause.lower() == "contradiction" or new_clause.lower() == "false":
                            if negated_label.lower() == "true":
                                final_answer = "True"
                            elif negated_label.lower() == "false":
                                final_answer = "False"
                            flag = 'false'
                        else: 
                            all_empty = "True"
                            for i, clause in enumerate(list_of_compelment):
                                if len(clause) > 0:
                                    selected_clause = list_of_compelment[i].pop(0)
                                    sos_list = list_of_sos[i]
                                    all_empty = "False"
                                    print("Check Cache: ", sos_list, "Current Complement: ", selected_clause)
                                    break
                                
                            if len(list_of_compelment) > 0 and all_empty == "True":
                                final_answer = "Unknown"
                                flag = 'false'
                            elif len(list_of_compelment) == 0:
                                final_answer = "Unknown"
                                flag = 'false'

                    search_round += 1
                    
                final_choice = self.final_process(final_answer)
                
                output = {'id': example['id'], 
                        'original_context': example['original_context'],
                        'question': example['question'], 
                        'translated_context': example['translated_context'],
                        'normalized_context': example['normalized_context'],
                        'normalized_conjecture': example['normalized_conjecture'],
                        'negated_label': negated_label,
                        'reasoning_step': self.list_to_indexed_string(reasoning_step),
                        'ground_truth': example['ground_truth'], 
                        'final_answer': final_answer,
                        'final_choice': final_choice,
                        'search_round': search_round,
                        'token_usage': token_usage,
                        }
                print("Token usage for the example", example['id'], ":", token_usage)
                print(output)
                return output
        
            except Exception as e:
                print('Error in generating example: ', example['id'])
                print(e)
                error = {'id': example['id']}
                return error
                
        def _atomic_write_json(path, data):
            d = os.path.dirname(path)
            os.makedirs(d, exist_ok=True)
            with tempfile.NamedTemporaryFile('w', encoding='utf-8', dir=d, delete=False) as tmp:
                json.dump(data, tmp, indent=2, ensure_ascii=False)
                tmp.flush()
                os.fsync(tmp.fileno())
                tmp_name = tmp.name
            os.replace(tmp_name, path)  
            
        def _safe_load_json_array(path):
            if not os.path.exists(path) or os.path.getsize(path) == 0:
                return []
            with open(path, 'r', encoding='utf-8') as f:
                try:
                    return json.load(f)
                except json.JSONDecodeError:
                    print(f"[WARN] {path} is corrupted JSON. Starting a new array.")
                    return []
            
        def save_output(output, is_error=False):
            if "llama" in self.model_name:
                model_name = 'llama'
            elif "Qwen3-14B" in self.model_name:
                model_name = 'qwen3-14b'
            elif "Qwen3-32B" in self.model_name:
                model_name = 'qwen3-32b'
            elif ":free" in self.model_name:
                model_name = self.model_name.replace(":free", "_free")
            elif "deepseek" in self.model_name:
                model_name = 'deepseek'
            else:
                model_name = self.model_name 
            
            file_name = f'{self.dataset_name}_{model_name}_search_negation_{self.negation}.json'
            file_path = os.path.join(self.save_path, self.dataset_name, file_name)
            print("Saving result (atomic) in path: ", file_path)
            lock = FileLock(file_path + ".lock")
            with lock:
                with self.file_lock:
                    try:
                        existing_data = _safe_load_json_array(file_path)
                        existing_data.append(output)
                        _atomic_write_json(file_path, existing_data)
                    except Exception as e:
                        print(f"Error in saving {'error' if is_error else ''} output: {e}")

        counter = 0
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.batch_num) as executor:
            future_to_example = {executor.submit(process_example, example, counter): example for example in raw_dataset}
            for future in concurrent.futures.as_completed(future_to_example):
                example = future_to_example[future]
                try:
                    output = future.result()
                    if 'error' in output:
                        save_output(output, is_error=True)
                    else:
                        print(f"Saving output for example: {output}")
                        save_output(output)
                except Exception as exc:
                    print(f'{example["id"]} generated an exception: {exc}')
                    traceback.print_exc()
                counter += 1
                            


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_path', type=str, default='./data')
    parser.add_argument('--dataset_name', type=str)
    parser.add_argument('--split', type=str)
    parser.add_argument('--save_path', type=str, default='./results')
    parser.add_argument('--demonstration_path', type=str, default='./icl_examples')
    parser.add_argument('--api_key', type=str)
    parser.add_argument('--model_name', type=str)
    parser.add_argument('--stop_words', type=str, default='------')
    parser.add_argument('--mode', type=str)
    parser.add_argument('--negation', type=str, default='False')
    parser.add_argument('--max_new_tokens', type=int)
    parser.add_argument('--base_url', type=str)
    parser.add_argument('--batch_num', type=int, default=1)
    parser.add_argument('--reasoning_effort', type=str, default='none')
    parser.add_argument('--search_round', type=int, default=10)
    args = parser.parse_args()
    return args

if __name__ == '__main__':
    args = parse_args()
    gpt3_problem_reduction = GPT3_Reasoning_Graph_Baseline(args)
    gpt3_problem_reduction.reasoning_graph_generation()